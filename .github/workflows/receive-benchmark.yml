name: Receive Benchmark

on:
  pull_request:
    types: [opened]
  issue_comment:
    types: [created]

jobs:
  run-benchmarks:
    runs-on: ubuntu-latest
    if: >
      github.event_name == 'pull_request' ||
      (github.event_name == 'issue_comment' &&
      startsWith(github.event.comment.body, 'run benchmarks') &&
      github.event.issue.pull_request)

    steps:
      # When triggered by comment, fetch corresponding PR info!
      - name: Get PR info for comment trigger
        id: pr_info
        # only applies to comment not new pr
        if: github.event_name == 'issue_comment'
        # action allows you to interact w/github api directly w/in workflow. fetches dynamic data
        uses: actions/github-script@v7
        with:
          script: |
            const pr = await github.rest.pulls.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.issue.number
            });
            core.setOutput('head', pr.data.head.ref)
            core.setOutput('base', pr.data.base.ref)
            core.setOutput('number', pr.data.number)
            core.setOutput('head_repo', pr.data.head.repo.full_name)

      # Set branch variables based on the event type.
      - name: Set branch variables
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "BASE_BRANCH=${{ github.event.pull_request.base.ref }}" >> $GITHUB_ENV
            echo "HEAD_BRANCH=${{ github.event.pull_request.head.ref }}" >> $GITHUB_ENV
            echo "${{ github.event.pull_request.number }}" > pr_number.txt
            echo "BASE_BRANCH=${{ github.event.pull_request.base.ref }}" > branch_info.txt
            echo "HEAD_BRANCH=${{ github.event.pull_request.head.ref }}" >> branch_info.txt
          else
            echo "BASE_BRANCH=${{ steps.pr_info.outputs.base }}" >> $GITHUB_ENV
            echo "HEAD_BRANCH=${{ steps.pr_info.outputs.head }}" >> $GITHUB_ENV
            echo "${{ steps.pr_info.outputs.number }}" > pr_number.txt
            echo "BASE_BRANCH=${{ steps.pr_info.outputs.base }}" > branch_info.txt
            echo "HEAD_BRANCH=${{ steps.pr_info.outputs.head }}" >> branch_info.txt
          fi

      - name: Debug environment variables
        run: |
          echo "BASE_BRANCH=$BASE_BRANCH"
          echo "HEAD_BRANCH=$HEAD_BRANCH"
          echo "PR_NUMBER:"
          cat pr_number.txt
          echo "Branch info:"
          cat branch_info.txt

      # Checkout the base (target) branch into a folder called "base"
      # (temp directories, only exist for duration of workflow run)
      - name: Checkout base branch
        uses: actions/checkout@v4
        with:
          ref: ${{ env.BASE_BRANCH }}
          path: base

      # Checkout the head (PR) branch into a folder called "head"
      - name: Checkout head branch
        uses: actions/checkout@v4
        with:
          # Use the head repository from pr_info output if available (issue_comment event),
          # otherwise fallback to the pull_request event payload.
          repository: ${{ steps.pr_info.outputs.head_repo || github.event.pull_request.head.repo.full_name }}
          ref: ${{ env.HEAD_BRANCH }}
          path: head

      # Set up Python.
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install pyperf
        run: |
          python -m pip install --upgrade pip
          pip install pyperf

      # should run benchmarks simulataneously?
      # Run the benchmark on the base branch.
      # these output files stored temporarily in the runner's workspace during the job's execution.
      # Once the job completes, the files are removed.
      - name: Run benchmark on base branch
        working-directory: base
        run: python benchmarks.py -o ../results-base.json

      # Run the benchmark on the head branch.
      - name: Run benchmark on head branch
        working-directory: head
        run: python benchmarks.py -o ../results-head.json

      # Compare the two benchmark result files and save the output in Markdown format.
      - name: Compare benchmarks
        run: python -m pyperf compare_to results-base.json results-head.json --table --table-format=md > comparison.txt

      - name: Upload benchmark artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            comparison.txt
            pr_number.txt
            branch_info.txt
